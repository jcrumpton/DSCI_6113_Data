{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "809b607f",
   "metadata": {},
   "source": [
    "# Python Machine Learning \n",
    "\n",
    "------\n",
    "## Tools to Learn / Use\n",
    "\n",
    "* Load and Clean Data  \n",
    "  Use Pandas unless you have a reason not to (data too large to fit in memory)\n",
    "* Explore Data  \n",
    "  Pandas, numpy, matplotlib, seaborn\n",
    "* Models  \n",
    "  SciPy, scikit-learn  \n",
    "  https://www.w3schools.com/python/python_ml_getting_started.asp  \n",
    "  https://www.kaggle.com/learn/intro-to-machine-learning  \n",
    "  Deep Learning: Pytorch, Keras, TensorFlow  \n",
    "  https://www.datacamp.com/tutorial/pytorch-vs-tensorflow-vs-keras\n",
    "  \n",
    "------\n",
    "## Good Reference Site\n",
    "\n",
    "* https://machinelearningmastery.com/start-here/\n",
    "* https://machinelearningmastery.com/start-here/#algorithms\n",
    "* https://machinelearningmastery.com/machine-learning-in-python-step-by-step/\n",
    "* https://machinelearningmastery.com/machine-learning-performance-improvement-cheat-sheet/\n",
    "\n",
    "------\n",
    "## Python Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c1b729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python version\n",
    "import sys\n",
    "print(f'Python: {sys.version}')\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "print(f'pandas: {pd.__version__}')\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "print(f'numpy: {np.__version__}')\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "print(f'matplotlib: {matplotlib.__version__}')\n",
    "\n",
    "# seaborn\n",
    "import seaborn as sns\n",
    "print(f'seaborn: {sns.__version__}')\n",
    "\n",
    "# scikit-learn\n",
    "import sklearn\n",
    "print(f'sklearn: {sklearn.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af5228e",
   "metadata": {},
   "source": [
    "-----\n",
    "## Problem\n",
    "\n",
    "We will use weather data from [MSU's North Farm](http://deltaweather.extension.msstate.edu/msu-north-farm-starkv) (next to the research park) to see if we can predict if it is going to rain tomorrow. \n",
    "\n",
    "We will use daily information from 6/1/2019 to 9/1/2024.\n",
    "\n",
    "| Attribute   | Definition    |\n",
    "|:------------|:--------------|\n",
    "|Date|                Record Date MM/DD/YYYY  |\n",
    "|Julian|              Record Julian Date NNN day of the year  |\n",
    "|AirTempMax|          Air Temperature Max (Degrees Fahrenheit F)  |\n",
    "|AirTempMin|          Air Temperature Min (Degrees Fahrenheit F)  |\n",
    "|AirTempObsv|         Air Temperature Observed (Degrees Fahrenheit F) This is an instantaneous reading for air temperature at 7:00:00 standard time.  |\n",
    "|HumidMax|            Relative Humidity Max (Percent)  |\n",
    "|HumidMin|            Relative Humidity Min (Percent)  |\n",
    "|HumidObsv|           Relative Humidity Observed (Percent) This is an instantaneous reading at 7:00:00 standard time.  |\n",
    "|Precip|              Precipitation (Inches n.nn) total rain fall that occurred for the day.  |\n",
    "|WindRun|             Wind Run (Miles) Wind travel for the day.  |\n",
    "|AvgWindSpeed|        Wind Speed (Miles Per Hour) Resultant / Average speed for the day.  |\n",
    "|WindDirection|       Wind Direction (Degrees) Resultant Direction for the day  |\n",
    "|SolarRadiation|      Solar Radiation (Langley's) that occurred for the day.  |\n",
    "|SoilTempMax|         Soil Temperature Max at 2 inch depth (Degrees Fahrenheit F) that occurred during the day.  |\n",
    "|SoilTempMin|         Soil Temperature Min at 4 inch depth (Degrees Fahrenheit F) that occurred during the day.  |\n",
    "|SoilTempObsv|        Soil Temperature Observed at 4 inch depth (Degrees Fahrenheit F) This is an instantaneous reading 4 inch soil temperature at 7:00:00 standard time.  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9d9509",
   "metadata": {},
   "source": [
    "-----\n",
    "## Load Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac0e666",
   "metadata": {},
   "outputs": [],
   "source": [
    "RawData = pd.read_csv('https://raw.githubusercontent.com/jcrumpton/DSCI_6113_Data/refs/heads/main/DS_Club/MSU_North_Farm.csv')\n",
    "display(RawData.info())\n",
    "RawData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdf08d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(RawData.isnull(), cbar=False);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd371e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = RawData.dropna().copy() \n",
    "Data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593bf6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Types\n",
    "\n",
    "Data['Date'] = pd.to_datetime(Data['Date'], format=\"%m/%d/%y\", errors='coerce') \n",
    "Data['AirTempMin'] = Data['AirTempMin'].astype(int)  \n",
    "Data['AirTempObsv'] = Data['AirTempObsv'].astype(int)  \n",
    "Data['HumidMin'] = Data['HumidMin'].astype(int)  \n",
    "Data['HumidObsv'] = Data['HumidObsv'].astype(int)  \n",
    "Data['SoilTempObsv'] = Data['SoilTempObsv'].astype(int)\n",
    "\n",
    "Data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dcd06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462d6fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c6e132",
   "metadata": {},
   "source": [
    "-----\n",
    "## Add Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa934306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Did it rain on a given day?\n",
    "\n",
    "Data['RainYN'] = np.where(Data['Precip'] > 0, 1, 0)\n",
    "Data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28b2c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift RainYN back one day to use as RainTom (rain tomorrow)\n",
    "Data['RainTom'] = Data['RainYN'].shift(-1)\n",
    "Data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e80a9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = Data.dropna()\n",
    "Data['RainTom'] = Data['RainTom'].astype(int)\n",
    "Data.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af1c2a1",
   "metadata": {},
   "source": [
    "-----\n",
    "## Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ac674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distributions\n",
    "print(Data.groupby('RainTom').size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e9d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(Data['RainTom'])\n",
    "sns.countplot(x ='RainTom', data = Data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe896ab5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(Data['Date'], Data['Precip'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13a9b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Month from Date\n",
    "Data['Month'] = pd.to_datetime(Data['Date']).dt.month\n",
    "\n",
    "# Move Month to next to Date\n",
    "column_to_move = Data.pop('Month')\n",
    "Data.insert(1, 'Month', column_to_move)\n",
    "\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0c41a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(Data.corr());\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c579f2d2",
   "metadata": {},
   "source": [
    "-----\n",
    "## Machine Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e050b176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def make_confusion_matrix(cf,\n",
    "                          group_names=None,\n",
    "                          categories='auto',\n",
    "                          count=True,\n",
    "                          percent=True,\n",
    "                          cbar=True,\n",
    "                          xyticks=True,\n",
    "                          xyplotlabels=True,\n",
    "                          sum_stats=True,\n",
    "                          figsize=None,\n",
    "                          cmap='Blues',\n",
    "                          title=None):\n",
    "    '''\n",
    "    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n",
    "    https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea\n",
    "    https://github.com/DTrimarchi10/confusion_matrix\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cf:            confusion matrix to be passed in\n",
    "\n",
    "    group_names:   List of strings that represent the labels row by row to be shown in each square.\n",
    "\n",
    "    categories:    List of strings containing the categories to be displayed on the x,y axis. Default is 'auto'\n",
    "\n",
    "    count:         If True, show the raw number in the confusion matrix. Default is True.\n",
    "\n",
    "    normalize:     If True, show the proportions for each category. Default is True.\n",
    "\n",
    "    cbar:          If True, show the color bar. The cbar values are based off the values in the confusion matrix.\n",
    "                   Default is True.\n",
    "\n",
    "    xyticks:       If True, show x and y ticks. Default is True.\n",
    "\n",
    "    xyplotlabels:  If True, show 'True Label' and 'Predicted Label' on the figure. Default is True.\n",
    "\n",
    "    sum_stats:     If True, display summary statistics below the figure. Default is True.\n",
    "\n",
    "    figsize:       Tuple representing the figure size. Default will be the matplotlib rcParams value.\n",
    "\n",
    "    cmap:          Colormap of the values displayed from matplotlib.pyplot.cm. Default is 'Blues'\n",
    "                   See http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                   \n",
    "    title:         Title for the heatmap. Default is None.\n",
    "\n",
    "    '''\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n",
    "    blanks = ['' for i in range(cf.size)]\n",
    "\n",
    "    if group_names and len(group_names)==cf.size:\n",
    "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
    "    else:\n",
    "        group_labels = blanks\n",
    "\n",
    "    if count:\n",
    "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
    "    else:\n",
    "        group_counts = blanks\n",
    "\n",
    "    if percent:\n",
    "        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n",
    "    else:\n",
    "        group_percentages = blanks\n",
    "\n",
    "    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n",
    "    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
    "    if sum_stats:\n",
    "        #Accuracy is sum of diagonal divided by total observations\n",
    "        accuracy  = np.trace(cf) / float(np.sum(cf))\n",
    "\n",
    "        #if it is a binary confusion matrix, show some more stats\n",
    "        if len(cf)==2:\n",
    "            #Metrics for Binary Confusion Matrices\n",
    "            precision = cf[1,1] / sum(cf[:,1])\n",
    "            recall    = cf[1,1] / sum(cf[1,:])\n",
    "            f1_score  = 2*precision*recall / (precision + recall)\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n",
    "                accuracy,precision,recall,f1_score)\n",
    "        else:\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n",
    "    else:\n",
    "        stats_text = \"\"\n",
    "\n",
    "\n",
    "    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n",
    "    if figsize==None:\n",
    "        #Get default figure size if not set\n",
    "        figsize = plt.rcParams.get('figure.figsize')\n",
    "\n",
    "    if xyticks==False:\n",
    "        #Do not show categories if xyticks is False\n",
    "        categories=False\n",
    "\n",
    "\n",
    "    # MAKE THE HEATMAP VISUALIZATION\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n",
    "\n",
    "    if xyplotlabels:\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label' + stats_text)\n",
    "    else:\n",
    "        plt.xlabel(stats_text)\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a636f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training & testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = Data[\"RainTom\"]\n",
    "X = Data.drop(columns=[\"Date\",\"RainTom\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "print (f'X_train: {X_train.shape} \\nX_test: {X_test.shape} \\ny_train: {y_train.shape} \\ny_test: {y_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e14c032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is our baseline? \n",
    "#\n",
    "# For this problem, what would happen if we always answered no?\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_baseline = np.zeros(len(y_test))\n",
    "print(f\"baseline model accuracy = {accuracy_score(y_test, y_pred_baseline)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ad150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Set regularization rate\n",
    "reg = 0.01\n",
    "\n",
    "\n",
    "# train a logistic regression model on the training set\n",
    "lr_clf = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr = lr_clf.predict(X_test)\n",
    "print(f\"LR model accuracy = {accuracy_score(y_test, y_pred_lr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0515ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "y_pred_dt = dt_clf.predict(X_test)\n",
    "\n",
    "print(f\"DT model accuracy = {accuracy_score(y_test, y_pred_dt)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5abc9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier().fit(X_train, y_train)\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "\n",
    "\n",
    "print(f\"RF model accuracy = {accuracy_score(y_test, y_pred_rf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45465ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a series containing features' importance from the model and feature names from the training data\n",
    "feature_importances = pd.Series(rf_clf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "\n",
    "# Plot a simple bar chart\n",
    "feature_importances.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7114e5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune hyperparameters - possible on all previous models\n",
    "\n",
    "if False:\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "    param_dist = {'n_estimators': range(50, 551, 50),\n",
    "                  'max_depth': range(1,10)}\n",
    "\n",
    "    # Create a random forest classifier\n",
    "    rf = RandomForestClassifier()\n",
    "\n",
    "    # Use random search to find the best hyperparameters\n",
    "    rand_search = RandomizedSearchCV(rf, \n",
    "                                     param_distributions=param_dist, \n",
    "                                     n_iter=10,\n",
    "                                     cv=5,\n",
    "                                     random_state=1)\n",
    "\n",
    "    # Fit the random search object to the data\n",
    "    rand_search.fit(X_train, y_train)\n",
    "\n",
    "    # Create a variable for the best model\n",
    "    best_rf = rand_search.best_estimator_\n",
    "\n",
    "    # Print the best hyperparameters\n",
    "    print('Best hyperparameters:',  rand_search.best_params_)\n",
    "\n",
    "    predictions = best_rf.predict(X_test)\n",
    "    accuracyScore = accuracy_score(y_test, predictions)\n",
    "    print(f\"Validation Accuracy with Tuned Random Forest: {accuracyScore}\")\n",
    "    \n",
    "# Best hyperparameters: {'n_estimators': 50, 'max_depth': 9}\n",
    "# Validation Accuracy with Tuned Random Forest: 0.6901041666666666\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172e30c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "labels = ['True Neg','False Pos','False Neg','True Pos']\n",
    "cf_matrix = confusion_matrix(y_test, y_pred_rf)\n",
    "make_confusion_matrix(cf_matrix, group_names=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be693a9",
   "metadata": {},
   "source": [
    "__Accuracy__ the ratio of true positives (TP) and true negatives (TN) to the total number of samples\n",
    "\n",
    "__Precision__ the ratio of TP to the sum of TP and false positives (FP)\n",
    "\n",
    "__Recall__ the ratio of TP to the sum of TP and false negatives (FN)\n",
    "\n",
    "__F1 Score__ the harmonic mean of precision and recall\n",
    "\n",
    "__Good and Bad Scores__\n",
    "* Accuracy above 0.90 (90%) is considered excellent because it means the model correctly predicts 9 out of 10 instances. \n",
    "\n",
    "* Precision, Recall, and F1 score above 0.80 (80%) indicate strong performance. In a cancer detection model, Precision of 0.80 means 80% of positive predictions are correct, while Recall of 0.80 means 80% of actual cancer cases are identified. An F1 score of 0.80 signifies a balanced trade-off between Precision and Recall, crucial when both false positives and negatives have significant consequences.\n",
    "\n",
    "* Scores below 0.50 (50%) are typically poor because they indicate that the model's performance is worse than random guessing. For instance, a credit card fraud detection system with an accuracy of 0.45 would be unreliable, as it's more likely to misclassify transactions than to correctly identify them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b488b406",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_cf_matrix = confusion_matrix(y_test, y_pred_baseline)\n",
    "make_confusion_matrix(baseline_cf_matrix, group_names=labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82121af5",
   "metadata": {},
   "source": [
    "-----\n",
    "## Where to go next....\n",
    "\n",
    "\n",
    "### Unbalanced Classes\n",
    "\n",
    ">  In a balanced dataset, the number of Positive and Negative labels is about equal. However, if one label is more common than the other label, then the dataset is imbalanced.\n",
    ">\n",
    "> Imbalanced datasets sometimes don't contain enough minority class examples to train a model properly.\n",
    "\n",
    "* https://developers.google.com/machine-learning/crash-course/overfitting/imbalanced-datasets\n",
    "* https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/\n",
    "* https://datasciencehorizons.com/handling-imbalanced-datasets-in-scikit-learn-techniques-and-best-practices/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4341a657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "#\n",
    "# Create oversampled training data set for minority class\n",
    "#\n",
    "X_oversampled, y_oversampled = resample(X[y == 1],  # Fewer rain days than 'no rain' days\n",
    "                                        y[y == 1],\n",
    "                                        replace=True,\n",
    "                                        n_samples=X[y == 0].shape[0],\n",
    "                                        random_state=1)\n",
    "\n",
    "# Append the oversampled minority class to the imbalanced data and related labels\n",
    "X_balanced = pd.concat([X[y == 0], X_oversampled])\n",
    "y_balanced = pd.concat([y[y == 0], y_oversampled])\n",
    "\n",
    "print(X_balanced.shape)\n",
    "print(y_balanced.shape)\n",
    "print(y_balanced.sum())\n",
    "\n",
    "# plt.hist(Data['RainTom'])\n",
    "sns.countplot(x ='RainTom', data = pd.DataFrame(y_balanced))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66484ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=1)\n",
    "print (f'X_train2: {X_train2.shape} \\nX_test2: {X_test2.shape} \\ny_train2: {y_train2.shape} \\ny_test2: {y_test2.shape}')\n",
    "\n",
    "bal_rf_clf = RandomForestClassifier().fit(X_train2, y_train2)\n",
    "y_pred_bal_rf =bal_rf_clf.predict(X_test2)\n",
    "\n",
    "print(f\"\\nRF model accuracy = {accuracy_score(y_test2, y_pred_bal_rf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69d655c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(y_test2, y_pred_bal_rf)\n",
    "make_confusion_matrix(cf_matrix, group_names=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae63cb3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# How does the new model trained on balanced date do on original test data?\n",
    "\n",
    "y_pred_bal_rf = bal_rf_clf.predict(X_test)\n",
    "\n",
    "#print(f\"RF model accuracy = {accuracy_score(y_test, y_pred_bal_rf)}\")\n",
    "cf_matrix = confusion_matrix(y_test, y_pred_bal_rf)\n",
    "make_confusion_matrix(cf_matrix, group_names=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109c9dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do balanced classes help with any classifier?\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Set regularization rate\n",
    "reg = 0.01\n",
    "\n",
    "# train a logistic regression model on the training set\n",
    "bal_lr_clf = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train2, y_train2)\n",
    "\n",
    "y_pred_bal_lr = bal_lr_clf.predict(X_test)\n",
    "print(f\"LR model accuracy = {accuracy_score(y_test, y_pred_bal_lr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98ed890",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Normalize / Scale Data\n",
    "\n",
    "> Many machine learning algorithms perform better when numerical input variables are scaled to a standard range.\n",
    "> \n",
    "> This includes algorithms that use a weighted sum of the input, like linear regression, and algorithms that use distance measures, like k-nearest neighbors.\n",
    "\n",
    "* https://machinelearningmastery.com/standardscaler-and-minmaxscaler-transforms-in-python/\n",
    "* http://archive.today/2024.09.25-195922/https://www.geeksforgeeks.org/data-normalization-with-python-scikit-learn/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8538e3dd",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Time Series Data\n",
    "\n",
    "We ignored any time series information when training our models.\n",
    "\n",
    "* https://builtin.com/data-science/time-series-forecasting-python\n",
    "* https://www.kaggle.com/code/kanncaa1/time-series-prediction-tutorial-with-eda\n",
    "* https://machinelearningmastery.com/start-here/#timeseries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d66849b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Predicting Rainfall Amount Categories\n",
    "\n",
    "* https://www.kaggle.com/code/satishgunjal/binary-multiclass-classification-using-sklearn#Multiclass-Classification\n",
    "* http://archive.today/2023.10.29-124722/https://towardsdatascience.com/comprehensive-guide-to-multiclass-classification-with-sklearn-127cc500f362\n",
    "* http://archive.today/2024.09.25-193956/https://www.geeksforgeeks.org/multiclass-classification-using-scikit-learn/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74434df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [-1.0, 0.00001, 0.39, 1.18, 100]\n",
    "labels = ['NoRain', 'Light', 'Moderate', 'Heavy']\n",
    "Data['RainCategory'] = pd.cut(Data['Precip'], bins, labels=labels)\n",
    "\n",
    "print(Data.groupby('RainCategory', observed=True).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f4f2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just pyplot\n",
    "# plt.hist(Data.RainCategory)\n",
    "\n",
    "# Using seaborn\n",
    "sns.countplot(x ='RainCategory', data = Data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e37f81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
